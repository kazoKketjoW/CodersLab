#The aim of the workshop is to collect information about non-governmental organizations 
#from any selected category. The information will come from the website https://spis.ngo.pl/. 
#The content of the page is generated on the server, so a combination of the requests 
#and bs4 libraries would be enough to extract most of the information, but to decrypt 
#email addresses you need to run JavaScript code - for this you will need the selenium library.


import requests
from bs4 import BeautifulSoup

URL = 'https://spis.ngo.pl/?cat[2384]=2753'

response = requests.get(URL)
category = BeautifulSoup(response.text, 'html.parser')

list_of_organizations = category.select_one('div.relative.mb5')

print('Znaleziono', len(list_of_organizations), 'organizacji')

organizations = list_of_organizations.select('div.pv3.bb')

urls = []

for organization in organizations:
    urls.append(
        organization.select_one('a')['href']
    )


from selenium.webdriver import Firefox  # możesz też użyć innej przeglądarki
from selenium.common.exceptions import NoSuchElementException

def get_organization_data(url):
    data = {
        'Adres email': '',
        'Adresy www': [],
        'Telefony': [],
        'KRS': '',
        'REGON': '',
        'NIP': '',
        'Rok powstania': '',
    }

    browser = Firefox(executable_path='geckodriver')  # jeśli trzeba, popraw ścieżkę
    browser.get(url)

    table = browser.find_element_by_css_selector('div.ba.b--light-gray.pa3.pt0')
    rows = table.find_elements_by_css_selector('div.f7-f6-xl')

    # Odwiedzamy po kolei każdy wiersz w tabeli
    for row in rows:
        try:
            # w każdym wierszu próbujemy znaleźć etykietę i wartość
            label = row.find_element_by_css_selector('.pr2')
            value = row.find_element_by_css_selector('.tr.grow-1')
        except NoSuchElementException:
            # pomijamy wiersze w których nie udało się znaleźć etykiety lub wartości
            # "continue" skacze na początek pętli for, z kolejnym wierszem w zmiennej "row"
            continue

        # Wiersz z adresem email jest wyjątkowy
        if label.text == 'E-mail':
            value.find_element_by_tag_name('span').click()
            data['Adres email'] = value.text

        # Kolejne wiersze to po prostu przepisywanie danych z przeglądarki do słownika
        elif label.text == 'Telefon':
            data['Telefony'].append(value.text)

        elif label.text == 'WWW':
            data['Adresy www'].append(value.text)

        elif label.text == 'KRS':
            data['KRS'] = value.text

        elif label.text == 'REGON':
            data['REGON'] = value.text

        elif label.text == 'NIP':
            data['NIP'] = value.text

        elif label.text == 'Rok powstania':
            data['Rok powstania'] = value.text

    # Zamykamy przeglądarkę, nie będzie już potrzebna
    browser.quit()
    return data


import csv

with open('report.csv', 'w', newline='') as csvfile:
    writer = csv.writer(csvfile)

    for organization_url in urls[:3]:
        data = get_organization_data(organization_url)
        writer.writerow([
            data['Adres email'],
            ' '.join(data['Adresy www']),
            ', '.join(data['Telefony']),
            data['KRS'],
            data['REGON'],
            data['NIP'],
            data['Rok powstania'],
        ])
